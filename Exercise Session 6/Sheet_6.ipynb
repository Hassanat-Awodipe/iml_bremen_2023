{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d886944",
   "metadata": {},
   "source": [
    "# Interpretable Machine Learning\n",
    "## Exercise Sheet 6: LIME\n",
    "## This exercise sheet covers lecture 6 on LIME\n",
    "Sophie Langbein (langbein@leibniz-bips.de)<br>\n",
    "Pegah Golchian (golchian@leibniz-bips.de)\n",
    "<hr style=\"border:1.5px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5c90a5",
   "metadata": {},
   "source": [
    "# Implementing LIME \n",
    "\n",
    "LIME, which stands for \"Local Interpretable Model-Agnostic Explanations,\" is an interpretable machine learning technique used to explain the predictions of complex models. LIME provides insight into how a machine learning model arrived at a particular prediction by approximating the model's behavior in the local vicinity of a specific data point or instance.\n",
    "\n",
    "In the following, you are guided to implement LIME to interpret machine learning models, specifically a support vector machine (SVM). For simplicity and the purpose of visualization we use only two (numeric) input features and explore LIME on a multiclass classification problem. We are considering the [wheat seeds dataset](https://archive.ics.uci.edu/dataset/236/seeds) from the UCI machine learning repository, which collects data for the purpose of classifying three different types of wheat Kernels (Kama = 0, Rosa = 1 and Canadian = 2),  and from it the `Area` and `Perimeter` features.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02037985",
   "metadata": {},
   "source": [
    "**a)** Inspect the following implemented helper functions `get_grid()`, `plot_grid()` and `plot_points_in_grid()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fb07d2",
   "metadata": {},
   "source": [
    "The function `get_grid()` prepares data to visualize the feature space. It creates a N × N grid, and every point in this grid is associated with a value. This value is obtained by the model’s predict method. It's designed for visualizing the behavior of a machine learning model over a grid of input values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1cd1bcd8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import random\n",
    "sys.path.insert(0, \".\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import itertools\n",
    "from sklearn import tree\n",
    "from math import sqrt\n",
    "import pandas as pd\n",
    "\n",
    "def get_grid(model, dataset, points_per_feature = 50):\n",
    "    \"\"\"\n",
    "    Retrieve grid data for plotting a two-dimensional graph with `points_per_feature` for each axis.\n",
    "    The space is created by the hyperparameters' lower and upper values. Only the first two input\n",
    "    labels are used.\n",
    "\n",
    "    Parameters:\n",
    "        model: A classifier that has a predict method. This is the machine learning model you want to evaluate.\n",
    "        dataset (pd.dataframe): An input dataset of interest in the form of a pandas dataframe.\n",
    "        points_per_feature: The number of points to generate for each feature dimension (default is 50).\n",
    "\n",
    "    Returns:\n",
    "        u (np.ndarray): An array containing the x-axis points, with a shape of (`points_per_feature`,).\n",
    "        v (np.ndarray):  An array containing the y-axis points, with a shape of (`points_per_feature`,).\n",
    "        z (np.ndarray): An array of values with a shape of (`points_per_feature`, `points_per_feature`). These values correspond to the model's predictions over the grid of input points.\n",
    "    \"\"\"\n",
    "\n",
    "    labels = dataset.columns.tolist()  # extracts input labels from the provided dataset.\n",
    "\n",
    "    x1 = np.linspace(min(dataset[labels[0]]),\n",
    "                     max(dataset[labels[0]]), points_per_feature) # create linearly spaced array x1 for the first input feature, using the lower and upper bounds of the first hyperparameter\n",
    "    x2 = np.linspace(min(dataset[labels[0]]),\n",
    "                     max(dataset[labels[0]]), points_per_feature) # create linearly spaced array x2 for the second input feature, using the lower and upper bounds of the second hyperparameter\n",
    "\n",
    "    X = [] \n",
    "    for x in itertools.product(x1, x2):\n",
    "        X.append(x) # create a list X containing all possible combinations of the x1 and x2 values using itertools.product, these combinations represent the grid points\n",
    "\n",
    "    X = np.array(X) # convert list X into a NumPy array X\n",
    "    y = model.predict(X) # predicts the corresponding output values for the grid points using the provided model\n",
    "\n",
    "    # Reshape all x\n",
    "    u = x1\n",
    "    v = x2\n",
    "    z = y.reshape(points_per_feature, points_per_feature).T # reshapes the predicted y values to create the final z array\n",
    "\n",
    "    return u, v, z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed9731f",
   "metadata": {},
   "source": [
    "The function `plot_grid()`, visualizes the prediction surface. It is used to create a plot with a color grid overlay. It visualizesg the output of the get_grid function described earlier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "043eef94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_grid(u, v, z, labels, title=None, embedded=False):\n",
    "    \"\"\"\n",
    "    Uses the grid data to add a color grid to the plot.\n",
    "\n",
    "    Parameters:\n",
    "        u (np.ndarray):  An array containing x-axis points, with a shape of (N,).\n",
    "        v (np.ndarray): An array containing y-axis points, with a shape of (N,).\n",
    "        z (np.ndarray): An array of color values with a shape of (N, N).\n",
    "        labels (list): A list containing labels for the x and y axes.\n",
    "        embedded (bool): A boolean that determines whether a new figure should be created for the plot (default is False).\n",
    "\n",
    "    Returns: \n",
    "        plt (matplotlib.pyplot or utils.styled_plot.plt): Plot with applied color grid.\n",
    "    \"\"\"\n",
    "\n",
    "    if not embedded:\n",
    "        plt.figure() # If embedded is False, a new figure is created using plt.figure(), this allows to start a new plot, especially when creating multiple plots in the same script or notebook\n",
    "\n",
    "    plt.xlabel(labels[0]) # x-axis label is set to 'labels[0]'\n",
    "    plt.ylabel(labels[1]) # x-axis label is set to 'labels[1]'\n",
    "    plt.title(title) # title of the plot is set to 'title'\n",
    "\n",
    "    plt.pcolormesh(u, v, z, cmap='viridis', shading='auto') # plt.pcolormesh() function is used to create the color grid based on the u, v, and z data\n",
    "    plt.colorbar() # colorbar is added to the plot using plt.colorbar() to provide a reference for the color values\n",
    "    plt.grid(alpha=0) # grid lines are turned off (made transparent) using plt.grid(alpha=0).\n",
    "\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddfd701",
   "metadata": {},
   "source": [
    "The created plot is an input to the function `plot_points_in_grid()`, which adds scatter points to the existing plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3bc8a836",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_points_in_grid(plt, Z=[], y=[], weights=None, colors={}, x_interest=None, size=8):\n",
    "    \"\"\"\n",
    "    Given a plot, add scatter points from `Z` and `x_interest`.\n",
    "\n",
    "    Parameters:\n",
    "        plt (matplotlib.pyplot or utils.styled_plot.plt): Plot with color grid, representing the plot to which scatter points will be added\n",
    "        Z (np.ndarray): Points with shape (?, 2) which should be added to the plot.\n",
    "        y (np.ndarray): Target values with shape (?,), of the points added to the plot, determines the colouring of points.\n",
    "        weights (np.ndarray): Normalized weights with shape (?,), determine the size of points in the plot.\n",
    "        colors (dict): A dictionary that maps target values to colors. It returns the color for a given target value.\n",
    "        x_interest (np.ndarray): Single point with shape (2,) whose prediciton we want to explain. If None (default) no point is added.\n",
    "        size (int): Default size of the markers/points. Default is 8.\n",
    "    \"\"\"\n",
    "\n",
    "    w = 1 # w is initialized with a value of 1, which represents the default weight for the size of the points\n",
    "\n",
    "    for y_ in list(set(y)): # iterate through unique target values in y\n",
    "        idx = np.where(y == y_)[0] # find the indices of the points in Z that have the current target value y_\n",
    "\n",
    "        color = \"black\" \n",
    "        if y_ in colors:\n",
    "            color = colors[y_] # If the target value exists in the colors dictionary, it uses the specified color; otherwise, it defaults to \"black.\"\n",
    "\n",
    "        if weights is not None:\n",
    "            w = weights[idx] # If weights are provided (not None), it assigns the weights for the current target to the variable w\n",
    "\n",
    "        plt.scatter(Z[idx, 0], Z[idx, 1], c=color, s=w*size, label=y_) # add scatter points to the plot.\n",
    "\n",
    "    if x_interest is not None:\n",
    "        plt.scatter([x_interest[0]], [x_interest[1]],\n",
    "                    c='red', s=size, label=\"POI\") # If x_interest is not None, it adds a red scatter point to the plot to represent the single point of interest (POI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59aab472",
   "metadata": {},
   "source": [
    "**b)** The first implementation task is to sample points, which are later used to train the local surrogate model. Complete `sample_points()` by randomly sampling from a uniform distribution. For sampling from the uniform distribution, consider the lower and upper bounds from the input datapoints."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8518f033",
   "metadata": {},
   "source": [
    "**Solution:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0784cca",
   "metadata": {},
   "source": [
    "The function `sample_points()` is used to generate random sample points for the first two features in a dataset, typically for the purpose of visualizing model behavior over a range of input values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1b2469f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_points(model, dataset, num_points, seed = 0):\n",
    "    \"\"\"\n",
    "    Samples points for the two first features.\n",
    "\n",
    "    Parameters:\n",
    "        model: A classifier that has a predict method, this is the machine learning model for which you want to sample points.\n",
    "        dataset (pd.dataframe): An input dataset of interest in the form of a pandas dataframe \n",
    "        num_points (int): An integer specifying how many points should be sampled.\n",
    "        seed (int): An integer used as the seed for the random number generator.\n",
    "\n",
    "    Returns:\n",
    "        Z (np.ndarray): Sampled data points with shape (num_points, 2). These are two-dimensional input points.\n",
    "        y (np.ndarray): Target values with shape (num_points,). These values are the model's predictions for the sampled data points.\n",
    "    \"\"\"\n",
    "\n",
    "    return Z, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc1a3a3",
   "metadata": {},
   "source": [
    "**c)** Weight Points\n",
    "\n",
    "Given a selected point $\\mathbf{x}$ and the sampled points $Z$ from the previous task, we now want to weight the points.\n",
    "Use the following equation with $d(x,z) = \\sqrt{(x-z)^2}$ as Euclidean distance and $\\sigma$ the kernel width to calculate the weight of a single point $\\mathbf{z} \\in Z$:\n",
    "\n",
    "$$\n",
    "\\phi_{\\mathbf{x}}(\\mathbf{z}) = \\exp(−d(\\mathbf{x}, \\mathbf{z})/\\sigma^2)\n",
    "$$\n",
    "\n",
    "To make plotting easier later on, the weights should be normalized between zero and one. Finally, return the normalized weights in `weight_points()`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204aa2bf",
   "metadata": {},
   "source": [
    "**Solution:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d48a8e",
   "metadata": {},
   "source": [
    "The function `weight_points()` calculates weights for a set of points based on their distances to a single point of interest (`x_interest`). The weights are calculated using an exponential kernel function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c1e5a4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_points(x_interest, Z, kernel_width=0.2):\n",
    "    \"\"\"\n",
    "    For every z in `Z` returns a weight depending on the distance to `x_interest`.\n",
    "\n",
    "    Parameters:\n",
    "        x_interest (np.ndarray): Single point with shape (2,) whose prediction we want to explain.\n",
    "        Z (np.ndarray): Points with shape (?, 2). These are the data points for which you want to calculate weights based on their distances to x_interest.\n",
    "        kernel_width (float): A floating-point value representing the kernel width. It is used to calculate the distance according to an exponential kernel function. The default value is 0.2.\n",
    "\n",
    "    Returns:\n",
    "        weights (np.ndarray): An array of normalized weights with shape (?,). These weights represent the importance or influence of each data point in Z based on its distance from x_interest.\n",
    "    \"\"\"\n",
    "\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806bf7c0",
   "metadata": {},
   "source": [
    "**d)** Fit Local Surrogate Model\n",
    "\n",
    "Finally, fit a decision tree using the training data and the weights. Return the fitted tree in the function\n",
    "`fit_explainer_model()`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1025c3aa",
   "metadata": {},
   "source": [
    "**Solution:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8bb1e3",
   "metadata": {},
   "source": [
    "The function `fit_explainer_model()` fits a decision tree regressor model to explain or approximate the relationship between input points (`Z`) and target values (`y`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "324f126a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_explainer_model(Z, y, weights=None, seed=0):\n",
    "    \"\"\"\n",
    "    Fits a decision tree.\n",
    "\n",
    "    Parameters:\n",
    "        Z (np.ndarray): Points with shape (?, 2), used to fit surrogate model.\n",
    "        y (np.ndarray): Target values with shape (?,). These are the values you want to explain or approximate.\n",
    "        weights (np.ndarray): Normalized weights with shape (?,). These weights represent the importance of each data point in Z. If not provided, the weights are not used.\n",
    "        seed (int): Seed for the decision tree.\n",
    "\n",
    "    Returns:\n",
    "        model (DecisionTreeRegressor): The fitted explainer model, which is a decision tree regressor.\n",
    "    \"\"\"\n",
    "\n",
    "    return explainer_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d210b5c4",
   "metadata": {},
   "source": [
    "**e)** Now we want to assemble all the functions written above to implement LIME. In a first step, import the data from `wheat_seeds.csv`. \n",
    "\n",
    "- Drop all rows that contain `NA` values\n",
    "- Drop all feature columns except `Area` and `Perimeter`. The outcome is recorded in the `Type` column.\n",
    "- Create training and testing sets\n",
    "- Fit a support vector machine to the training dataset, with the `gamma` parameter set to `auto`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e9e1dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "735b4094",
   "metadata": {},
   "source": [
    "**f)** Use the settings given below to plot the decision surface of the SVM model. Select a meaningful point of interest. \n",
    "\n",
    "Hint: Use the `get_grid()` and `plot_grid()` functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512e35dc",
   "metadata": {},
   "source": [
    "**Solution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "469bd035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "x_interest = np.array([?, ?]) # set your own point of interest\n",
    "points_per_feature = 50\n",
    "n_points = 1000\n",
    "labels = dataset.columns.tolist() \n",
    "colors = {\n",
    "        0: \"purple\",\n",
    "        1: \"green\",\n",
    "        2: \"orange\",\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9294a286",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bf9f7015",
   "metadata": {},
   "source": [
    "**g)** Generate sample points around the two features`Area` and `Perimenter` using the `sample_points()` function. Then plot the SVM decision surface and the Sampled Points using the `plot_grid()`and the `plot_points_in_grid()` functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77bc9f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "89c58094",
   "metadata": {},
   "source": [
    "**h)** Weight the sampled points according to their distance to the point of interest `x_interest` using the `weight_points()` function. Then plot the SVM decision surface together with the weighted sampled points again using the `plot_grid()`and the `plot_points_in_grid()` functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61ce292",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "302de07c",
   "metadata": {},
   "source": [
    "**i)** Now fit a local surrogate model in the form of a decision tree using the `fit_explainer_model()`function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "1ddba989",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "321028b6",
   "metadata": {},
   "source": [
    "**j)** Plot the decision surface of the surrogate decision tree model, including the point of interest and the weighted sampled points comparison purposes. Is the local surrogate model a good fit? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f5423a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "364e662d",
   "metadata": {},
   "source": [
    "<hr style=\"border:1.5px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc76a75",
   "metadata": {},
   "source": [
    "# Application of LIME\n",
    "\n",
    "For a practical application of LIME, consider the Seoul Bike Sharing dataset, which was taken from the UCI machine learning repository https://archive.ics.uci.edu/dataset/560/seoul+bike+sharing+demand. The dataset contains count of public bicycles rented per hour in the Seoul Bike Sharing System, with corresponding weather data and holiday information. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee1d81e",
   "metadata": {},
   "source": [
    "**a)** Import the Seoul Bike Sharing dataset from `SeoulBikeData.csv`. Use one hot encoding to encode all categorical features. Then split the data into training and test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911ab813",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b221ff39",
   "metadata": {},
   "source": [
    "**b)** For an application of LIME, fit a gradient boosting regression model to the preprocessed data. Calculate the $R^2$ on the test set to evaluate the model fit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4c7b79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d89baa4a",
   "metadata": {},
   "source": [
    "**c)** Use the LIME from the `lime` package to give local explanations of the first and the 11th instance of the test data. Use a local ridge regression (default) as a surrogate model and discretize the data. Then visualize the results using an adequate plot using using 8 features. Interpret the results. Why could the local explanations be problematic?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4722b33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8ebf0e76",
   "metadata": {},
   "source": [
    "**d)** Now replace the default ridge regression with a decision tree as a surrogate model. Plot the results for the first instance and compare them to the results from **c)**. What do you notice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c983a9eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d71ab466",
   "metadata": {},
   "source": [
    "**e)** Now use KernelSHAP to generate an explanation for the first instance. Visualize the results in a force plot. Compare them to the results to the LIME for the first instance. What do you notice? Why is this happening? In your opinion, which method is more useful SHAP or LIME? Briefly explain your reasons.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76eba46b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d0f7f809",
   "metadata": {},
   "source": [
    "<hr style=\"border:1.5px solid gray\"> </hr>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
