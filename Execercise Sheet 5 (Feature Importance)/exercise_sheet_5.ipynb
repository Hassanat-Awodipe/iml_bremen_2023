{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61bbda8e",
   "metadata": {},
   "source": [
    "# Interpretable Machine Learning\n",
    "## Exercise Sheet: 5 Feature Importance\n",
    "### Presentation date: 04.12.2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86496ad5",
   "metadata": {},
   "source": [
    "# Excercise 1: Permutation feature importance (PFI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17bc33f",
   "metadata": {},
   "source": [
    "Permutation Feature Importance is one of the oldest and most widely used IML techniques. It is defined as\n",
    "\\begin{align}\n",
    "\\widehat{P F I}_S = \\frac{1}{m} \\sum_{k=1}^m \\mathcal{R}_{\\text{emp}} (\\hat{f},\\tilde{\\mathcal{D}}_{(k)}^S)-\n",
    "\\mathcal{R}_{\\text{emp}} (\\hat{f},\\mathcal{D})\n",
    "\\end{align}\n",
    "\n",
    "where $\\tilde{\\mathcal{D}}_{(k)}^S$ is the dataset where features $S$ were replaced with a perturbed version that preserves the variables marginal distribution $P (X_S )$. We can approximate sampling from the marginal distribution by random permutation\n",
    "of the original feature’s observations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34fdcd8",
   "metadata": {},
   "source": [
    "**a)** PFI has been criticized to evaluate the model on unrealistic observations. \n",
    "\n",
    "**1.** Describe in a few words why this extrapolation happens. \n",
    "\n",
    "**2.** Think of an illustrative example.\n",
    "\n",
    "**3.** Under a (seldom realistic) assumption PFI does not suffer from the extrapolation issue. What is that\n",
    "assumption? Briefly explain why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff5ca7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a811f7c6",
   "metadata": {},
   "source": [
    "**b)** As in the previous excercise sheet 4, we use the [students' performance](https://archive.ics.uci.edu/dataset/320/student+performance) dataset from UCI machine learning respository. \n",
    "\n",
    "**1.** As in the **SHAP values** excercise load `student-mat.csv`and fit a random forest classifier. (You can copy your solution from a) & b) from the last excercise).\n",
    "\n",
    "**2.** Calculate the permutation feature importance (PFI) from a predefined python package. For example from `sklearn.inspection`.\n",
    "\n",
    "**3.** Visualize the PFI results, i.e. plot the mean variable importances and the according standard deviations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256b2ebe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bf1897a0",
   "metadata": {},
   "source": [
    "**c)** Interpret PFI.\n",
    "\n",
    "**1.** Interpret the PFI result from b). What insight into model and data do we gain?\n",
    "\n",
    "- Which features are (mechanistically) used by the model for it’s prediction?\n",
    "- Which features are (in)dependent with $Y$?\n",
    "- Which features are (in)dependent with its covariates?\n",
    "- Which features are dependent with $Y$, given all covariates?\n",
    "\n",
    "**2.** Compare your results with the SHAP bar plot *g)* from last excercise. For that also plot only the 10 most important features for better visualisation. (This means also to order the feature importance.) Do they detect the same important features? What is the big difference between SHAP feature importance and PFI?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7fd639",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "318f076b",
   "metadata": {},
   "source": [
    "**d)** Implement PFI yourself and apply it to the `student-mat.csv` dataset. Compare your results with the plot from b). Since we have so many features, reduce your plot with the 10 most important features.   \n",
    "\n",
    "In order to make your code reusable for the upcoming exercises, break down the implementation into three functions:\n",
    "\n",
    "- `pfi_fname` which returns the PFI for a feature `fname`\n",
    "- `fi` a function that computes the importances for all features using a single-feature importance function\n",
    "such as `pfi_fname`\n",
    "- `n_times` a function that repeats the computation $n$ times and returns mean and standard deviation of\n",
    "the importance values\n",
    " \n",
    "Hint: By passing the single-feature importance function as an argument you can reuse fi and n times later\n",
    "on for other feature importance method and only have to adjust fi fname accordingly. In order to allow for\n",
    "different function signatures you may use `f(*args, **kwargs)` in python (more info [here](https://realpython.com/python-kwargs-and-args/)) and `f(...)` in\n",
    "R (more info [here](https://stackoverflow.com/questions/8165837/how-to-pass-a-function-and-its-arguments-through-a-wrapper-function-in-r-simila))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb7e04ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pfi_fname(fname, predict, score, X_test, y_test, *args):\n",
    "    \"\"\"Function that returns the pfi for a single feature.\n",
    "    Args:\n",
    "    fname: feature of interest name\n",
    "    predict: prediciton function\n",
    "    score: performance metric\n",
    "    X_test: data for the evaluation\n",
    "    y_test: respective labels\n",
    "    *args: further arguments (which are ignored)\n",
    "    Returns:\n",
    "    performance: performance metric\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04d913f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fi_naive(fi_fname, predict, score, X_test, y_test, *args, **kwargs):\n",
    "    \"\"\"Naive feature importance implementation.\n",
    "    Args:\n",
    "    perf_pert: function that returns performance for some perturbation.\n",
    "    predict: prediction function\n",
    "    score: performance metric\n",
    "    X_test: test data for the evaluation\n",
    "    y_test: respective labels\n",
    "    2\n",
    "    *args: further arguments, e.g. training data (can be ignored here)\n",
    "    Returns:\n",
    "    results: relevance for each feature (in the order of X_test.columns)\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4eafcd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocess as mp\n",
    "def n_times(n, method, *args, return_raw=False, **kwargs):\n",
    "    \"\"\"Parallelized implementation for the repeated evaluation of fi.\n",
    "    Args:\n",
    "    n: number of repetitions\n",
    "    method: feature importance method.\n",
    "    args: all further arguments that are required for the method\n",
    "    return_raw: Whether only the aggregation (mean, stdd) or also the raw results are returned\n",
    "    Returns:\n",
    "    mean_fi, std_fi, (raw results)\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5965187",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b41b8ee0",
   "metadata": {},
   "source": [
    "**e)** \n",
    "**1.** Plot the correlation structure of the data. What insight into the relationship of the features with $y$ do we gain by looking at the correlation structure of the covariates in addition to the PFI? \n",
    "\n",
    "**2.** In which two variables is the extrapolation issue most prominent in this dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbca92a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b361d204",
   "metadata": {},
   "source": [
    "# Excercise 2: Conditional sampling based feature importance techniques\n",
    "\n",
    "Conditional Feature Importance has been suggested as an alternative to Permutation Feature Importance.\n",
    "\n",
    "**a)** Implement a linear Gaussian conditional sampler. For conditional feature importance the sampler must be\n",
    "able to learn Gaussian conditionals with multivariate conditioning set and univariate target.\n",
    "Advice: For multivariate Gaussian data, the conditional distributions can be derived analytically from mean\n",
    "vector and covariance matrix, see [here](https://en.wikipedia.org/wiki/Multivariate_normal_distribution#Conditional_distributions).\n",
    "\n",
    "**1.** Given the decomposition of the covariance matrix as\n",
    "\n",
    "$\\begin{align}\n",
    "\\Sigma= \\begin{bmatrix}\n",
    "\\Sigma_{11} & \\Sigma_{12}\\\\\n",
    "\\Sigma_{21} & \\Sigma_{22}\n",
    "\\end{bmatrix} \\qquad \\text{with sizes} \\qquad \n",
    "\\begin{bmatrix}\n",
    "q \\times q & q \\times (N-q)\\\\\n",
    "(N-q) \\times q & (N-q) \\times (N-q)\n",
    "\\end{bmatrix}\n",
    "\\end{align}$\n",
    "\n",
    "the distribution of $X_1$ conditional on $X_2 = a$ is the multivariate normal $\\mathcal{N} (\\bar{\\mu}, \\Sigma)$\n",
    "\n",
    "$\\begin{align}\n",
    "\\bar{\\mu} &= \\mu_1 + \\Sigma_{12} \\Sigma_{22}^{-1}(a-\\mu_2)\\\\\n",
    "\\bar{\\Sigma} &= \\Sigma_{11} - \\Sigma_{12} \\Sigma_{22}^{-1} \\Sigma_{21}\n",
    "\\end{align}$\n",
    "\n",
    "As the target here is univariate $q = 1$ holds. Learn a function that returns the conditional mean and\n",
    "covariance structure given specific values for the conditioning set.\n",
    "\n",
    "**2.** Then write a function that takes the conditional mean and covariate structure and allows to sample\n",
    "from the respective (multivariate) Gaussian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6db7b71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "593a3e2c",
   "metadata": {},
   "source": [
    "**b)** Using your sampler, write a function that computes CFI. You may assume that the data is multivariate\n",
    "Gaussian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221bddc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "483ffeda",
   "metadata": {},
   "source": [
    "**c)** Apply CFI to the dataset and model from Exercise 1. Interpret the result: which insights into model and\n",
    "data are possible? Compare the result with PFI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cc6ce8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "19b83159",
   "metadata": {},
   "source": [
    "# Excercise 3 LOCO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62aa692a",
   "metadata": {},
   "source": [
    "We can also assess the importance of a feature by refitting the model with and without access to the feature of\n",
    "interest and compare the respective predictive performances. The method is also referred to as so-called leave-one-\n",
    "covariate-out (LOCO) importance.\n",
    "\n",
    "**(a)** Implement LOCO.\n",
    "\n",
    "**(b)** Apply LOCO to the dataset from Exercise 1 (use a random forest model again).\n",
    "\n",
    "**(c)** Interpret the result (insight into model and data). Compare the result to PFI and CFI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880312c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
