{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "469dd78a",
   "metadata": {},
   "source": [
    "# Interpretable Machine Learning\n",
    "## Exercise Sheet 9: Ante-hoc Interpretability Methods\n",
    "## This exercise sheet covers lecture 9 on Ante-hoc Interpretability Metods\n",
    "Sophie Langbein (langbein@leibniz-bips.de)<br>\n",
    "Pegah Golchian (golchian@leibniz-bips.de)\n",
    "<hr style=\"border:1.5px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9317032e",
   "metadata": {},
   "source": [
    "\n",
    "# Instance-wise Feature Selection with Select and Predict Models\n",
    "\n",
    "Select and predict style models are composed of a selector, that predicts a binary mask over the input features of each instance, and a predictor, that consumes the masked input to make a final prediction. Unfortunately, binary masking is a non-differentiable operation. This makes it hard to train such models end-to-end. A workaround is the so-called pipeline setup, where selector and predictor are trained independently. In addition to a label for each instance to train the predictor, this requires groundtruth explanations.\n",
    "\n",
    "`text: A gorgeous musical I watched at the palace cinema` <br>\n",
    "`label: 1 (positive)` <br>\n",
    "`rationale: [0,1,0,0,0,0,0,0,0]`\n",
    "\n",
    "`text: What a bad drama` <br>\n",
    "`label: 0 (negative)` <br>\n",
    "`rationale: [0,0,1,0,0]`\n",
    "\n",
    "In this exercise, the goal is to train and run such a pipeline model on a movie review sentiment classification dataset. The dataset also includes rationale annotations, i.e. highlights that represent important tokens in form of a binary mask. We want to use these as groundtruth explanations to train the selector model. Here are two examples:\n",
    "\n",
    "Note that the 1s in the rationales highlight the tokens that are important for sentiment classification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc911c39",
   "metadata": {},
   "source": [
    "**Selector Model**\n",
    "\n",
    "The goal of the selector model is to predict these rationale masks. In our case, the selector is a token classifier, that predicts either 0 or 1 for each token in the sequence. For this exercise, we choose `DistilBERT` for both the selector and the predictor model. Running the selector consists of the following steps:\n",
    "\n",
    "1. First, the input text has to be tokenized. That means, the input text tokens are mapped to a sequence of integer input ids. For BERT-style models, a few special tokens are added: Each sequence starts with a `[CLS]` token and ends with a `[SEP]` token. Since all sequences in one batch must be of the same length, a `[PAD]` token is used to pad shorter sequences to the same length as the longest one in the batch. For a batch containing our two examples, the tokenized input could look like this:\n",
    "\n",
    "    `text: [CLS] A gorgeous musical I watched at the palace cinema [SEP]` <br>\n",
    "    `input ids: [101, 2, 876, 1098, 5, 66, 78, 134, 867, 555, 102]` <br>\n",
    "    `attention mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]` <br>\n",
    "    `rationale: [−100,0,1,0,0,0,0,0,0,0,−100]` <br>\n",
    "\n",
    "    `text: [CLS] What a bad drama . [SEP] [PAD] [PAD] [PAD] [PAD]` <br>\n",
    "    `input ids: [101, 44, 2, 11, 43, 3, 102, 0, 0, 0, 0]` <br>\n",
    "    `attention mask: [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]` <br>\n",
    "    `rationale: [−100,0,0,1,0,0,−100,−100,−100,−100,−100]` <br>\n",
    "\n",
    "The special tokens `[CLS]`, `[SEP]`, `[PAD]` are represented by the special input ids `101`, `102`, and `0`, respectively. Also, the rationale is aligned to the tokenized input ids and is filled with `-100` to indicate that the label and the token at that index should be ignored when computing the loss or performance metrics.\n",
    "\n",
    "2. The model takes as input the input ids and attention masks. The output obtained by running a forward pass are logits of shape (batch size, 2, sequence length). By normalization, one can obtain the probabilities for each of the two classes for each token in the input from these logits.\n",
    "\n",
    "3. After obtaining the predictions, the input tokens for which the predicted label is `0` can be masked by replacing the corresponding token ids with the token id of the `[MASK]` token.\n",
    "\n",
    "4. Finally, the input ids can be converted back to string text using a decode function, that inverts the tokenization process. Any `[CLS]`, `[SEP]`, `[PAD]` tokens can be dropped. An example result could be:\n",
    "\n",
    "    `masked text: [MASK] gorgeous [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK]`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2de99ea",
   "metadata": {},
   "source": [
    "**Predictor Model**\n",
    "\n",
    "The predictor model is a sequence classification model, as its goal is to predict a single label for every input sequence. Given a movie review text that was masked by the selector model, it predicts either 1 (`positive`) or 0 (`negative`). The inputs must be tokenized in the same way as for the selector. The model output is now of shape (batch size, 2). Again, these are the logits for each class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9fd1e2",
   "metadata": {},
   "source": [
    "<hr style=\"border:1.5px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ae2edf",
   "metadata": {},
   "source": [
    "## Training the Selector Model\n",
    "\n",
    "In this exercise, we build on pretrained models. Yet, the selector needs some finetuning to the new task of extracting explanation masks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f2e687",
   "metadata": {},
   "source": [
    "**a)** As a first step, make sure the required packages are installed in your python environment. Then import the required packages and functions as described below and familiarize yourself with the imports from `utils` using the documentation from below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4602e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required packages\n",
    "numpy\n",
    "pandas\n",
    "pytest\n",
    "torch\n",
    "transformers\n",
    "pytreebank\n",
    "tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efb4bc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os  # noqa\n",
    "sys.path.insert(0, \"\")  # noqa\n",
    "\n",
    "import torch\n",
    "torch.manual_seed(0)\n",
    "\n",
    "from utils.dataset import (\n",
    "        SentimentRationaleDataset,\n",
    "        tokenize,\n",
    "        decode,\n",
    "        pad_token_id,\n",
    "        mask_token_id,\n",
    "        cls_token_id,\n",
    "        sep_token_id,\n",
    "        _custom_collate\n",
    "    )\n",
    "from classifiers.distilbert import Selector, Predictor\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fde855",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Some documentation for imports:\n",
    "\n",
    "\n",
    "tokenize (function):\n",
    "    Use this function to tokenize the input text, and optionally align the corresponding rationales.\n",
    "\n",
    "    Parameters:\n",
    "        text (List[List[str]]):\n",
    "            A batch of text as returned by the dataloaders.\n",
    "        Optional: rationale (List[List[int]]):\n",
    "            A batch of rationale masks as returned by the dataloaders.\n",
    "            Required when using the rationales as labels, as they have to remain aligned with the text tokens.\n",
    "    Returns:\n",
    "        tokenized_inputs (dict):\n",
    "            A dict containing the tokenized text (key='input_ids'), an attention_mask (key='attention_mask') and aligned rationales (key='rationales) if passed.\n",
    "            Rationale labels that belong to tokens that not belong to the text are labeld with -100.\n",
    "\n",
    "\n",
    "decode (function):\n",
    "    Use this function to turn tokenized input_ids back to text.\n",
    "\n",
    "    Parameters:\n",
    "        input_ids (torch.tensor): A batch of input ids.\n",
    "    Returns:\n",
    "        text (str): decoded text.\n",
    "\n",
    "\n",
    "{pad, mask, cls, sep}_token_id (int):\n",
    "    The token_id representing the [PAD], [MASK], [CLS], [SEP] token, respectively.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d305db",
   "metadata": {},
   "source": [
    "**b)** The goal of this exercise is to complete the `train_selector_model` function that trains the token classification head of the `DistilBERT` model for one epoch and then validates the model. The function should take the selector model, the training instances and the validation instances as inputs and return average training loss, average training accuracy over batches as well as the average validation loss and accuracy over all batches. For more detailed descriptions, read the documentation in the function below. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88ee22d",
   "metadata": {},
   "source": [
    "**i)** In a first step complete the `train_one_epoch` function. The function trains the selector model for one epoch. The training can be summarized in the following steps:\n",
    "\n",
    "- first, set the selector model to training mode using `selector_model.train()`\n",
    "- then, iterate over all batches in the training dataloader `dl_train`\n",
    "- tokenize each batch and rationales using the `tokenize` function from `utils`\n",
    "- set gradients to zero\n",
    "- perform a forward pass through then selector model using the tokenized inputs, and obtain model output using `selector_model(input_ids='input_ids', attention_mask='attention_mask')` with the input_ids and attention mask obtained from the `tokenize` function\n",
    "- compute the cross-entropy loss using `loss = torch.nn.functional.cross_entropy()`\n",
    "- perform a backward pass & optimization step using `loss.backward()` and `optimizer.step()`\n",
    "- finally compute the training accuracy for each batch\n",
    "- for the training accuracy obtain the predicted labels (binary) from the selector model using `.argmax(1)` (important: exclude tokens with label -100 (used for padding))\n",
    "- obtain the true labels of all tokens != -100, from `tokenized_batch['rationales'] \n",
    "- then, compute the accuracy by comparing predicted and true labels             "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fc4ff3",
   "metadata": {},
   "source": [
    "**ii)** In a second step complete the `validate` function. The function validates the selector model. The validation can be summarized in the following steps:\n",
    "\n",
    "- first, set the selector model to evluation mode using `selector_model.eval()`\n",
    "- then, iterate over all batches in the training dataloader `dl_val`\n",
    "- tokenize each batch and rationales using the `tokenize` function from `utils`\n",
    "- switch off gradient tracking using `with torch.no_grad()`\n",
    "- compute the model output from the selector model without gradient tracking, using `selector_model(input_ids='input_ids', attention_mask='attention_mask')` with the input_ids and attention mask obtained from the `tokenize` function\n",
    "- compute the cross-entropy loss using `loss = torch.nn.functional.cross_entropy()`\n",
    "- for the validation accuracy obtain the predicted labels (binary) from the selector model using `.argmax(1)` (important: exclude tokens with label -100 (used for padding))\n",
    "- obtain the true labels of all tokens != -100, from `tokenized_batch['rationales'] \n",
    "- then, compute the accuracy by comparing predicted and true labels             "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d95091c",
   "metadata": {},
   "source": [
    "**iii)** In a final step call the `train_one_epoch` and the `validate` functions to obtain both training and validation losses and accuracies for one epoch. Compute their respective means and return them as  `epoch_train_loss`, `epoch_train_acc`, `epoch_val_loss`,`epoch_val_acc`. \n",
    "\n",
    "**Hint:** To retain the compatibility with other PyTorch operations, convert the Python lists `train_losses`, `train_accs`, `val_losses`, `val_accs` to PyTorch tensors using `torch.tensor` and then retrieve the mean as a Python float."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e9458b",
   "metadata": {},
   "source": [
    "**Solution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5542d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_selector_model(selector_model, dl_train, dl_val):\n",
    "    \"\"\"\n",
    "    Trains the given selector model for one epoch, then validates the model.\n",
    "    Essentially, the goal of the selector model is to predict a mask such that only the important tokens are revealed.\n",
    "    For example, for the positive movie review\n",
    "        `A georgeous movie .`\n",
    "    the prediction could be `0, 1, 0, 0`.\n",
    "    For each token in the input sequence, the model predicts 0 if the token should be masked and 1 if the token should be revealed.\n",
    "    In this exercise, we train the selector in a supervised manner, using annotated rationale data (in the form of binary masks).\n",
    "\n",
    "    The dataloaders for the selector return batches in the form of dicts, with the following structure:\n",
    "        'text': List[List[str]]:\n",
    "            A batch of movie review text.\n",
    "            Each review is a List of tokens.\n",
    "        'rationale': List[List[int]]:\n",
    "            A batch of rationale masks.\n",
    "            Each rationale is a List representing a binary mask over tokens (length = num of tokens in text).\n",
    "        'label': List[int]: \n",
    "            A batch of labels, either 0 (negative) or 1 (positive).\n",
    "            Not relevant for training the selector, as here the rationale masks are used as groundtruth.\n",
    "\n",
    "    Parameters:\n",
    "        selector_model (Selector): \n",
    "            A token classification model based on DistilBERT.\n",
    "            For each token in the input sequence, the model predicts whether it should be masked (0) or not (1).\n",
    "            The selector_model is also a torch.nn.Module, so you can call its forward method as\n",
    "                selector_model(input_ids, attention_mask)\n",
    "            Both of these inputs can be created by applying the `tokenize` function on a batch returned by the dataloaders.\n",
    "\n",
    "        dl_train (torch.utils.data.DataLoader): The dataloader containing the training instances in batches.\n",
    "        dl_val (torch.utils.data.DataLoader): The dataloader containing the validation instances in batches.\n",
    "    \n",
    "    Returns:\n",
    "        epoch_train_loss (float): The average loss over the batches seen during training.\n",
    "        epoch_train_acc (float): The average accuracy over the batches seen during training.\n",
    "        epoch_val_loss (float): The average loss over the batches seen during validation.\n",
    "        epoch_val_acc (float): The average accuracy over the batches seen during validation.\n",
    "    \"\"\"\n",
    "    optimizer = torch.optim.AdamW(selector_model.parameters(), lr=1e-5) #  initialize an AdamW optimizer for the parameters of the selector_model with a learning rate of 1e-5.\n",
    "\n",
    "    def train_one_epoch():\n",
    "        \"\"\"\n",
    "\n",
    "        Returns:\n",
    "            train_losses (List[float]): A list containing the loss of each batch seen during training.\n",
    "            train_accs (List[float]) A list containing the accuracy of each batch seen during training.\n",
    "\n",
    "        Hints: \n",
    "            - Use `.item()` before appending the loss / accuracy of a batch to the corresponding list.\n",
    "            - torch.nn.functional.cross_entropy already automatically ignores inputs labeled with -100.\n",
    "            - When computing accuracy, only include the input_ids belonging to the text.\n",
    "                These are all the tokens for which the tokenized rationale mask is != -100.\n",
    "            \n",
    "        \"\"\"\n",
    "        # fill in \n",
    "        \n",
    "        return train_losses, train_accs # return the list of training losses and accuracies\n",
    "\n",
    "    def validate():\n",
    "        \"\"\"\n",
    "\n",
    "        Returns:\n",
    "            val_losses (List[float]): A list containing the loss of each batch seen during validation.\n",
    "            val_accs (List[float]) A list containing the accuracy of each batch seen during validation.\n",
    "\n",
    "        Hint: See train_one_epoch(). Optionally use `with torch.no_grad():` to disable gradient tracking (not needed during eval).\n",
    "            \n",
    "        \"\"\"\n",
    "        # fill in \n",
    "        \n",
    "        return val_losses, val_accs # return the list of validation losses and accuracies\n",
    "        \n",
    "      # fill in \n",
    "    \n",
    "      return epoch_train_loss, epoch_train_acc, epoch_val_loss, epoch_val_acc # return the computed metrics for one epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6b8716",
   "metadata": {},
   "source": [
    "**c)** Complete the `select` function, which uses the selector model to predict the masked text for all instances in the dataloader. \n",
    "\n",
    "The function should perform the following steps: \n",
    "\n",
    "- iterate over batches in the provided dataloader\n",
    "- for each batch tokenize the text and the rationale\n",
    "- disable gradient tracking during inference using `with torch.no_grad()`\n",
    "- obtain the model's output by performing a forward pass on the tokenized batch using `selector_model(input_ids='input_ids', attention_mask='attention_mask')` as before\n",
    "- compute the predicted mask by selecting the index with the maximum value along the second dimension of the model's output using `.argmax(1)` \n",
    "- then iterate over instances in the batch\n",
    "- retrieve the input_ids for the current instance\n",
    "- create a mask of relevant tokens (excluding special tokens, [CLS], [SEP], and [PAD] (all input_ids that are not `cls_token_id`, `sep_token_id`, `pad_token_id` from `utils`)\n",
    "- apply the predicted mask to the relevant input_ids, by replacing all input_ids for which the mask=0 with `mask_token_id` from `utils`\n",
    "- decoding the input_ids back to (now masked) text using the `decode` function from `utils`\n",
    "- append the masked text (split into tokens) and the label for the current instance to a dictionary\n",
    "- by iterating over all batches create a list containing one dictionary for every instance in the dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37ca4d6",
   "metadata": {},
   "source": [
    "**Solution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45f22005",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select(selector_model, dl):\n",
    "    \"\"\"\n",
    "\n",
    "    Parameters:\n",
    "        selector_model (Selector): \n",
    "            The selector model used for prediction.\n",
    "        dl (torch.utils.data.DataLoader):\n",
    "            The dataloader containing the instances to predict.\n",
    "\n",
    "    Returns:\n",
    "        selections (List[dict]):\n",
    "            A list containing one dict for every instance in the dataloader.\n",
    "            Each dict has two keys:\n",
    "                'text': A list of tokens (as in the dataloader). Some tokens are replaced with the mask token [MASK]\n",
    "                'label': The label of the instance (as in the dataloader).\n",
    "\n",
    "    \"\"\"\n",
    "    # fill in \n",
    "    \n",
    "    return selector_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a12ef7d",
   "metadata": {},
   "source": [
    "**c)** Complete the `predict` function, which predicts the sentiment label for the instances in the dataloader.\n",
    "\n",
    "The function should perform the following steps: \n",
    "\n",
    "- iterate over batches in the provided dataloader\n",
    "- for each batch tokenize the text and the rationale\n",
    "- disable gradient tracking during inference using `with torch.no_grad()`\n",
    "- obtain the output of the predictor model by performing a forward pass on the tokenized batch using `predictor_model(input_ids='input_ids', attention_mask='attention_mask')` as before\n",
    "- compute the predicted labels by selecting the index with the maximum value along the second dimension of the model output using `.argmax(1)` \n",
    "- collect the predictions in a list and return them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4501ff",
   "metadata": {},
   "source": [
    "**Solution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b113d74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(predictor_model, dl):\n",
    "    \"\"\"\n",
    "\n",
    "    Parameters:\n",
    "        predictor_model (Predictor):\n",
    "            A sequence classification model based on DistilBERT.\n",
    "            For each input sequence, the model predicts whether it is negative (0) or positive (1).\n",
    "            The predictor model is also a torch.nn.Module, so you can call its forward method as\n",
    "                predictor_model(input_ids, attention_mask)\n",
    "            Both of these inputs can be created by applying the `tokenize` function on a batch returned by the dataloaders.\n",
    "\n",
    "        dl (torch.utils.data.DataLoader):\n",
    "            The dataloader containing the instances to predict.\n",
    "            The instances in the dataloader are the results of the `select` function.\n",
    "\n",
    "    Returns:\n",
    "        predictions (List[int]): A list containing the predicted labels (0 or 1) for each instance in the dataloader.\n",
    "    \"\"\"\n",
    "    # fill in \n",
    "    \n",
    "    return predictions # returns the list of predicted labels for each instance in the dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea3fbec",
   "metadata": {},
   "source": [
    "**d)** Now we want to use the above functions to perform instance-wise feature selection. For that purpose first load training and validation data and get training and validation dataloaders as described below. Also initialize the selector model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1662fd",
   "metadata": {},
   "source": [
    "**Solution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "401b3160",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e2ea05cade24af78ae726dfe59670ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/563 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93c927e48bb44087a03a3fb06b5e1813",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/265M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get training and validation set\n",
    "ds_train = SentimentRationaleDataset('train', limit=1000)\n",
    "ds_val = SentimentRationaleDataset('dev', limit=100)\n",
    "\n",
    "# get training and validation dataloaders\n",
    "dl_train = ds_train.get_dataloader()\n",
    "dl_val = ds_val.get_dataloader()\n",
    "\n",
    "# initialize an instance of the selector class and assign it to the variable selector_model\n",
    "selector_model = Selector()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcebebc",
   "metadata": {},
   "source": [
    "**e)** Train the token classification head of the `DistilBERT` model for one epoch and then validate the model, by computing training and validation loss and accuracy using the `train_selector_model` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a561fa7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "83699caf",
   "metadata": {},
   "source": [
    "**f)** Predict the masked text for all instances in the dataloader using the `select`function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49442c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "35f54f40",
   "metadata": {},
   "source": [
    "**g)** Predict the sentiment label for the instances in the dataloader using the `predict`function. For this purpose the predictor model needs to be initialized and a DataLoader (dl) for the validation dataset needs to be created. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca68638f",
   "metadata": {},
   "source": [
    "**Solution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc31a718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize an instance of the predictor class and assign it to the variable predictor_model\n",
    "predictor_model = Predictor()\n",
    "\n",
    "# create a DataLoader for validation dataset\n",
    "dl = torch.utils.data.DataLoader(ds_val_masked, batch_size=8, collate_fn=_custom_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc13c2f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9f38b336",
   "metadata": {},
   "source": [
    "**h)** Run the following code to see whether the instance-wise feature selection process was successfull. The code should print the first for instances of the movie review dataset, then the selection with the masked text, the prediction of the predictor model of the corresponding instance and its groundtruth. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6156adf3",
   "metadata": {},
   "source": [
    "**Solution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ceb0d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=' * 80)\n",
    "print('Examples:')\n",
    "for i in range(0, 4):\n",
    "    print('-' * 80)\n",
    "    print(f'Instance: {\" \".join(ds_val[i][\"text\"])}')\n",
    "    print(f'Selection: {\" \".join(ds_val_masked[i][\"text\"])}')\n",
    "    print(f'Prediction: {\"positive\" if predictions[i] else \"negative\"}')\n",
    "    print(f'Groundtruth: {\"positive\" if ds_val[i][\"label\"] else \"negative\"}')\n",
    "    print('-' * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486df44c",
   "metadata": {},
   "source": [
    "<hr style=\"border:1.5px solid gray\"> </hr>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
